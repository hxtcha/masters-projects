{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "understood-mention",
   "metadata": {
    "deletable": false,
    "heading_collapsed": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b0361f750f04a80b0f110dd32cbbff71",
     "grade": false,
     "grade_id": "cell-ebbe47193fc775ea",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Spoken digit classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outdoor-angle",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2ad4c0b492b3b273e15f673b2cf8ce01",
     "grade": false,
     "grade_id": "cell-Q3Info",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "For this task, you can use all concepts that you have learned throughout this module in order to obtain a classifier that can determine which digit is heard in an audio file. While you are allowed to use different libraries for visualisation/audio files manipulations purposes, you can only use NumPy to program your classifier; tools from libraries such as **SciPy/SciKit-learn/Keras/PyTorch etc. are not allowed**. However, you may wish to use such libraries as **Librosa, python_speech_features**, etc for audio files manipulation. Please note that your classfier will be tested and compared with classifiers from other students. Experimenting with differnt model approaches, regularisation models & parameters and hyperparameter-tuning strategies such as cross validation is therefore highly recommended.\n",
    "\n",
    "**The dataset structure**: the dataset consists of recordings of spoken digits in **.wav** files at $8$kHz. The recordings are trimmed so that they have near minimal silence at the beginnings and ends. There are $6$ different speakers who pronounce each digit from $0$ to $9$ for $50$ times. Each audio file has a name of the form $\\left\\{digit\\right\\}\\_\\left\\{speaker\\,\\,name\\right\\}\\_\\left\\{attempt\\,\\,number\\right\\}.wav$. The data is split into training and validation sets with the ratio $80:20$.\n",
    "\n",
    "There are many ways of representing/visualising the audio data and here in the project we will work with a spectral characteristic known as mel-frequency cepstral coefficients (MFCC) (see [Wikipedia](https://en.wikipedia.org/wiki/Mel-frequency_cepstrum) for details). As we will see later this can be treated as an pixel image.\n",
    "\n",
    "We begin by loading the Audio MNIST training dataset that is taken from this [source](https://github.com/Jakobovski/free-spoken-digit-dataset):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f682bb9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-24T15:19:24.812670Z",
     "start_time": "2021-12-24T15:19:22.590046Z"
    },
    "deletable": false,
    "editable": false,
    "hideCode": false,
    "hidePrompt": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "41a9de69bde502a38c9676288f044059",
     "grade": false,
     "grade_id": "cell-LibrariesImport",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "regulated-medicine",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-24T13:24:42.670292Z",
     "start_time": "2021-12-24T13:24:36.399243Z"
    },
    "deletable": false,
    "editable": false,
    "hidden": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4635a0b1bfb7deb73bb8ddaf9323bb73",
     "grade": false,
     "grade_id": "cell-Q3DataLoad",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "audio_mnist_training_mfccs = np.genfromtxt(\n",
    "    'AudioMNIST/MFCC/Training/training_mfccs.txt')\n",
    "audio_mnist_training_labels = np.genfromtxt(\n",
    "    'AudioMNIST/MFCC/Training/training_labels.txt').reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d866a272",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_mnist_testing_mfccs = np.genfromtxt(\n",
    "    'AudioMNIST/MFCC/Testing/testing_mfccs.txt')\n",
    "audio_mnist_testing_labels = np.genfromtxt(\n",
    "    'AudioMNIST/MFCC/Testing/testing_labels.txt').reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84d42290",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_mnist_mfccs = np.r_[audio_mnist_training_mfccs,audio_mnist_testing_mfccs]\n",
    "audio_mnist_labels = np.r_[audio_mnist_training_labels,audio_mnist_testing_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d762771d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_dictionary ={}\n",
    "for i in range(len(audio_mnist_mfccs)):\n",
    "    key = int(audio_mnist_labels[i])\n",
    "    if key not in classes_dictionary.keys():\n",
    "        classes_dictionary[key] = audio_mnist_mfccs[i]\n",
    "    else:\n",
    "        classes_dictionary[key] = np.c_[classes_dictionary[key],audio_mnist_mfccs[i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sexual-kingdom",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "20e06c99af6a706ea6ce6be788b7f576",
     "grade": false,
     "grade_id": "cell-Q3DataInfo",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Remark:** Please note that the corresponding data folder contains two types of data:\n",
    "- raw audio files (folder Audio)\n",
    "- audio files Mel-frequency cepstral coefficients (folder MFCC) saved in txt file\n",
    "\n",
    "In the above we use Mel-frequency cepstral coefficients as the data representation of audio. You are allowed to use alternative representations if you wish. If you do so, please comment in your report on why have you selected another model and what have you achieved by doing that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "related-couple",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e2b64ad258c73bd8f727914e630c525f",
     "grade": false,
     "grade_id": "cell-Q3Demostration",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Below we take a random element of a training data and demonstrate how the MFCC can be visualised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "systematic-humidity",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-24T13:28:35.048413Z",
     "start_time": "2021-12-24T13:28:34.495690Z"
    },
    "deletable": false,
    "editable": false,
    "hidden": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8e95213d81c32cc8b761fccc40fc8663",
     "grade": false,
     "grade_id": "cell-Q3DemostrationPlot",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Audio MNIST training set contains 2400 samples with 900 MFC coefficients each.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.tight_layout(*, pad=1.08, h_pad=None, w_pad=None, rect=None)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"The Audio MNIST training set contains {s} samples with {p} MFC coefficients each.\".format(s = audio_mnist_training_mfccs.shape[0], \\\n",
    "        p = audio_mnist_training_mfccs.shape[1]))\n",
    "\n",
    "item_number = np.random.randint(low=0, high=2400)\n",
    "\n",
    "plt.imshow(audio_mnist_training_mfccs[item_number].reshape(30, 30), cmap='hot')\n",
    "plt.title(\"This is the {n}th audio sample of the Audio MNIST training set. The corresponding label is {l}\".format( \\\n",
    "            n= item_number, l=int(audio_mnist_training_labels[item_number][0])))\n",
    "plt.tight_layout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invisible-differential",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ccb2879e7e27d4259fc388f88a6c9e2f",
     "grade": false,
     "grade_id": "cell-Q3CodeInfo1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Use the following space to write your codes. It should be possible to reproduce results that are shown in your report with the codes that are described here. You can outsource functions into separate files if you find that this tidies up your notebook. Any additional libraries that you want to use (e.g. for visualisations etc.) can be loaded here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bce4871",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0519f09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardise(data_matrix):\n",
    "    mean = np.mean(data_matrix, axis=0)\n",
    "    std = np.std(data_matrix, axis=0)\n",
    "    standardised_matrix = (data_matrix - mean) / std\n",
    "    return standardised_matrix, mean, std\n",
    "\n",
    "def linear_regression_data(data_inputs):\n",
    "    X = np.ones((len(data_inputs),1))\n",
    "    X = np.c_[X,data_inputs]\n",
    "    return X\n",
    "\n",
    "def linear_model_function(data_matrix, weights):\n",
    "#     print(data_matrix@weights)\n",
    "    return data_matrix@weights\n",
    "\n",
    "def gradient_descent(objective,gradient,initial_weights,step_size=1,no_of_iterations=100,print_output=10):\n",
    "    objective_values = []\n",
    "    weights = np.copy(initial_weights)\n",
    "    objective_values.append(objective(weights))\n",
    "    for counter in range(no_of_iterations):\n",
    "        weights -= step_size * gradient(weights)\n",
    "        objective_values.append(objective(weights))\n",
    "        if (counter + 1) % print_output == 0:\n",
    "            print(\"Iteration {k}/{m}, objective = {o}.\".format(k=counter+1, \\\n",
    "            m=no_of_iterations, o=objective_values[counter]))\n",
    "    print(\"Iteration completed after {k}/{m}, objective = {o}.\".format(k=counter + 1, \\\n",
    "                                                                    m=no_of_iterations, o=objective_values[counter]))\n",
    "    return weights, objective_values\n",
    "\n",
    "def gradient_descent_v2(objective, gradient, initial_weights, step_size=1, no_of_iterations=100, print_output=10, \\\n",
    "                        tolerance=1e-6):\n",
    "    objective_values = []\n",
    "    weights = np.copy(initial_weights)\n",
    "    objective_values.append(objective(weights))\n",
    "    for counter in range(no_of_iterations):\n",
    "        weights -= step_size * gradient(weights)\n",
    "        objective_values.append(objective(weights))\n",
    "        if (counter + 1) % print_output == 0:\n",
    "            print(\"Iteration {k}/{m}, objective = {o}.\".format(k=counter+1, \\\n",
    "            m=no_of_iterations, o=objective_values[counter]))\n",
    "        if np.linalg.norm(gradient(weights)) <= tolerance:\n",
    "            break\n",
    "    print(\"Iteration completed after {k}/{m}, objective = {o}.\".format(k=counter + 1, \\\n",
    "                                                                    m=no_of_iterations, o=objective_values[counter]))\n",
    "    return weights, objective_values\n",
    "\n",
    "def proximal_gradient_descent(objective,gradient,proximal_map,initial_weights,step_size=1,no_of_iterations=1000,\n",
    "                              print_output=100):\n",
    "    objective_values = []\n",
    "    weights = np.copy(initial_weights)\n",
    "    objective_values.append(objective(weights))\n",
    "    for counter in range(no_of_iterations):\n",
    "        weights = proximal_map(weights - step_size * gradient(weights))\n",
    "        objective_values.append(objective(weights))\n",
    "        if (counter + 1) % print_output == 0:\n",
    "            print(\"Iteration {k}/{m}, objective = {o}.\".format(k=counter+1, \\\n",
    "            m=no_of_iterations, o=objective(weights)))\n",
    "    print(\"Iteration completed after {k}/{m}, objective = {o}.\".format(k=counter + 1, \\\n",
    "                                                                    m=no_of_iterations, o=objective_values[counter]))\n",
    "    return weights, objective_values\n",
    "\n",
    "def soft_thresholding(argument, threshold):\n",
    "#     print('argument: ', argument)\n",
    "#     print('threshold: ', threshold)\n",
    "    return np.sign(argument) * np.maximum(0, np.abs(argument) - threshold)\n",
    "\n",
    "def softmax_function(argument, axis=None):\n",
    "    if axis == None:\n",
    "        output = np.exp(argument - np.max(argument))\n",
    "        output = output / np.sum(output)\n",
    "    else:\n",
    "        output = np.exp(argument - np.expand_dims(np.max(argument, axis), axis))\n",
    "        output = output / np.expand_dims(np.sum(output, axis), axis)\n",
    "    return output\n",
    "\n",
    "def one_hot_vector_encoding_mnist(labels):\n",
    "    mnist_labels = np.concatenate(labels)\n",
    "    no_of_classes = int(np.max(mnist_labels)) + 1\n",
    "    output = np.zeros((len(mnist_labels), no_of_classes))\n",
    "    output[np.arange(len(mnist_labels)).astype(int), mnist_labels.astype(int)] = 1\n",
    "    return output\n",
    "\n",
    "def grid_search(objective, grid):\n",
    "    values = np.array([])\n",
    "    for point in grid:\n",
    "        values = np.append(values, objective(point))\n",
    "    print(values)\n",
    "#         print(grid[np.argmin(values)])\n",
    "    return grid[np.argmin(values)]\n",
    "\n",
    "def classification_accuracy(true_labels, recovered_labels):\n",
    "#     equal_labels = np.array([true_labels[n] == recovered_labels[n] for n in range(len(recovered_labels))])\n",
    "    equal_labels = true_labels == recovered_labels\n",
    "#     print(np.mean(equal_labels))\n",
    "    return np.mean(equal_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7d5127",
   "metadata": {},
   "source": [
    "## Neural Network Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2bfb475e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_forward_propagation(data_input, weight_matrix):\n",
    "    no_of_layers = weight_matrix.shape[0]\n",
    "    nn_state = np.empty(2*no_of_layers+1, dtype = np.ndarray)\n",
    "    nn_state[0] = data_input\n",
    "    for l in range(no_of_layers):\n",
    "        nn_state[2*l+1] = linear_model_function(linear_regression_data(nn_state[2*l]),weight_matrix[l])\n",
    "        nn_state[2*l+2] = softmax_function(nn_state[2*l+1], axis = 1)\n",
    "    return nn_state\n",
    "\n",
    "def nn_back_propagation(data_input, weight_matrix, one_hot_vector_encodings):\n",
    "        \n",
    "    nn_state = nn_forward_propagation(data_input, weight_matrix)\n",
    "    no_of_layers = weight_matrix.shape[0]\n",
    "    \n",
    "    full_X_gradient = np.empty(no_of_layers, dtype = np.ndarray)\n",
    "    full_W_gradient = np.empty(no_of_layers, dtype = np.ndarray)\n",
    "    full_Z_gradient = np.empty(no_of_layers, dtype = np.ndarray)\n",
    "    \n",
    "    full_Z_gradient[no_of_layers-1] = nn_state[-1] - one_hot_vector_encodings\n",
    "    \n",
    "    for layer in range(no_of_layers,0,-1):\n",
    "        full_W_gradient[layer-1] = linear_regression_data(nn_state[2*layer-2]).T @ full_Z_gradient[layer-1]\n",
    "        full_X_gradient[layer-1] = full_Z_gradient[layer-1] @ (np.delete(weight_matrix[layer-1],(0), axis = 0)).T\n",
    "    \n",
    "        if layer != 1:\n",
    "            product_matrix = full_X_gradient[layer-1]*nn_state[2*layer-2]\n",
    "            sigma_matrix = np.sum(product_matrix,axis=1)\n",
    "            full_Z_gradient[layer-2] = product_matrix - (nn_state[2*layer-2].T*sigma_matrix).T\n",
    "        \n",
    "    return full_W_gradient\n",
    "\n",
    "def nn_cost_function(data_input, weight_matrix, one_hot_vector_encodings):\n",
    "    nn_state = nn_forward_propagation(data_input, weight_matrix)\n",
    "    return np.sum(np.log(np.sum(np.exp(nn_state[-2]), axis=1))\\\n",
    "                  - np.sum(one_hot_vector_encodings*nn_state[-2], axis=1))\n",
    "\n",
    "def nn_prediction_function(data_input, weight_matrix):\n",
    "    nn_state = nn_forward_propagation(data_input, weight_matrix)\n",
    "    return np.argmax(nn_state[-1], axis=1)\n",
    "\n",
    "def nn_cost_function_ridge(data_input, weight_matrix, one_hot_vector_encodings,regularisation_parameter):\n",
    "    return nn_cost_function(data_input, weight_matrix, one_hot_vector_encodings) + \\\n",
    "                                                    (regularisation_parameter/2)*np.linalg.norm(weight_matrix)**2\n",
    "\n",
    "def nn_cost_function_LASSO(data_input, weight_matrix, one_hot_vector_encodings,regularisation_parameter):\n",
    "    return nn_cost_function(data_input, weight_matrix, one_hot_vector_encodings) + \\\n",
    "                                                    regularisation_parameter*np.sum(np.abs(weight_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1da4e639",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MNIST_multinomial_logistic_model_function(input_data, input_labels):\n",
    "    '''Multinomial Logistic Regression.'''\n",
    "\n",
    "    audio_mnist_data, _, _ = standardise(input_data)\n",
    "\n",
    "    audio_mnist_data_matrix = linear_regression_data(audio_mnist_data)\n",
    "\n",
    "    audio_mnist_OHV = one_hot_vector_encoding_mnist(input_labels)\n",
    "\n",
    "    audio_mnist_objective = lambda weight_matrix: multinomial_logistic_regression_cost_function(\\\n",
    "                                                                                    audio_mnist_data_matrix,\\\n",
    "                                                                                    weight_matrix,\\\n",
    "                                                                                    audio_mnist_OHV)\n",
    "\n",
    "\n",
    "    audio_mnist_gradient = lambda weight_matrix: multinomial_logistic_regression_gradient(\\\n",
    "                                                                                    audio_mnist_data_matrix,\\\n",
    "                                                                                    weight_matrix,\\\n",
    "                                                                                    audio_mnist_OHV)\n",
    "\n",
    "    audio_mnist_initial_weights = np.zeros(shape=(len(audio_mnist_data_matrix.T),10))\n",
    "\n",
    "    audio_mnist_step_size = (3.9*len(audio_mnist_data_matrix))/(np.linalg.norm(\\\n",
    "                                                                                audio_mnist_data_matrix)**2)\n",
    "\n",
    "    audio_mnist_optimal_weights, audio_mnist_objective_values = gradient_descent_v2(audio_mnist_objective, \\\n",
    "                                                                                    audio_mnist_gradient, \\\n",
    "                                                                                    audio_mnist_initial_weights,\\\n",
    "                                                                                    audio_mnist_step_size, \\\n",
    "                                                                                    no_of_iterations=2000, \\\n",
    "                                                                                    print_output=2000, \\\n",
    "                                                                                    tolerance=1e-3)\n",
    "    \n",
    "    labels = multinomial_prediction_function(audio_mnist_data_matrix, audio_mnist_optimal_weights)\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3aac7565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MNIST_ridge_multinomial_logistic_model_function(input_data, input_labels, regularisation_parameter=1e-4):\n",
    "    \n",
    "    audio_mnist_ridge_regularisation_parameter = regularisation_parameter\n",
    "\n",
    "    audio_mnist_data = standardise(input_data)[0]\n",
    "\n",
    "    audio_mnist_ridge_training_data_matrix = linear_regression_data(audio_mnist_data) \n",
    "\n",
    "    audio_mnist_OHV = one_hot_vector_encoding_mnist(input_labels)\n",
    "\n",
    "    audio_mnist_ridge_objective = lambda weight_matrix: ridge_multinomial_logistic_regression_cost_function(\\\n",
    "                                                                      audio_mnist_ridge_training_data_matrix,\\\n",
    "                                                                      weight_matrix,audio_mnist_OHV,\\\n",
    "                                                                      audio_mnist_ridge_regularisation_parameter) \n",
    "\n",
    "    audio_mnist_ridge_gradient = lambda weight_matrix: ridge_multinomial_logistic_regression_gradient(\\\n",
    "                                                                        audio_mnist_ridge_training_data_matrix,\\\n",
    "                                                                        weight_matrix,audio_mnist_OHV, \\\n",
    "                                                                        audio_mnist_ridge_regularisation_parameter) \n",
    "\n",
    "    audio_mnist_ridge_initial_weights = np.zeros(shape=(len(audio_mnist_ridge_training_data_matrix.T),10)) \n",
    "\n",
    "    audio_mnist_ridge_step_size = (3.9*len(audio_mnist_ridge_training_data_matrix))/(np.linalg.norm(\\\n",
    "                                                                        audio_mnist_ridge_training_data_matrix)**2)\n",
    "\n",
    "    audio_mnist_ridge_optimal_weights, audio_mnist_ridge_objective_values = gradient_descent_v2(\\\n",
    "                                                                        audio_mnist_ridge_objective, \\\n",
    "                                                                        audio_mnist_ridge_gradient, \\\n",
    "                                                                        audio_mnist_ridge_initial_weights,\\\n",
    "                                                                        step_size = audio_mnist_ridge_step_size, \\\n",
    "                                                                        no_of_iterations=2000, \\\n",
    "                                                                        print_output=2000, \\\n",
    "                                                                        tolerance=1e-5)\n",
    "    \n",
    "    labels = multinomial_prediction_function(audio_mnist_ridge_training_data_matrix, \\\n",
    "                                             audio_mnist_ridge_optimal_weights)\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5d34a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MNIST_LASSO_multinomial_logistic_model_function(input_data, input_labels, regularisation_parameter):\n",
    "    '''LASSO Multinomial Logistic Regression.'''\n",
    "\n",
    "    audio_mnist_LASSO_regularisation_parameter = regularisation_parameter\n",
    "    \n",
    "    audio_mnist_data, _, _ = standardise(input_data)\n",
    "\n",
    "    audio_mnist_LASSO_data_matrix = linear_regression_data(audio_mnist_data) \n",
    "\n",
    "    audio_mnist_OHV = one_hot_vector_encoding_mnist(input_labels)\n",
    "\n",
    "    audio_mnist_LASSO_objective = lambda weight_matrix: lasso_logistic_regression_cost_function(\\\n",
    "                                                                      audio_mnist_LASSO_data_matrix,\\\n",
    "                                                                      weight_matrix,audio_mnist_OHV,\\\n",
    "                                                                      audio_mnist_LASSO_regularisation_parameter) \n",
    "        \n",
    "    audio_mnist_LASSO_gradient = lambda weight_matrix: multinomial_logistic_regression_gradient(\\\n",
    "                                                                        audio_mnist_LASSO_data_matrix,\\\n",
    "                                                                        weight_matrix,audio_mnist_OHV) \n",
    "\n",
    "    audio_mnist_LASSO_initial_weights = np.zeros(shape=(len(audio_mnist_LASSO_data_matrix.T),10)) \n",
    "\n",
    "    audio_mnist_LASSO_step_size = (3.9*len(audio_mnist_LASSO_data_matrix))/(np.linalg.norm(\\\n",
    "                                                                        audio_mnist_LASSO_data_matrix)**2)\n",
    "\n",
    "    audio_mnist_proximal_map = lambda weights: soft_thresholding(weights, audio_mnist_LASSO_regularisation_parameter*\\\n",
    "                                                                 audio_mnist_LASSO_step_size)\n",
    "\n",
    "    audio_mnist_LASSO_optimal_weights, audio_mnist_LASSO_objective_values = proximal_gradient_descent(\\\n",
    "                                                                audio_mnist_LASSO_objective, \\\n",
    "                                                                audio_mnist_LASSO_gradient, audio_mnist_proximal_map,\\\n",
    "                                                                audio_mnist_LASSO_initial_weights, \\\n",
    "                                                                audio_mnist_LASSO_step_size, \\\n",
    "                                                                no_of_iterations=2000, print_output=2000)\n",
    "    \n",
    "    labels = multinomial_prediction_function(audio_mnist_LASSO_data_matrix, audio_mnist_LASSO_optimal_weights)\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fd5c49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MNIST_ridge_multinomial_logistic_model_function_nn(input_data, input_labels, regularisation_parameter=1e-4):\n",
    "\n",
    "    no_of_layers = 1\n",
    "\n",
    "    audio_mnist_ridge_regularisation_parameter = regularisation_parameter\n",
    "\n",
    "    audio_mnist_training_data = standardise(input_data)[0]\n",
    "\n",
    "    audio_mnist_training_data_matrix = linear_regression_data(audio_mnist_training_data)\n",
    "\n",
    "    audio_mnist_OHV = one_hot_vector_encoding_mnist(input_labels)\n",
    "\n",
    "\n",
    "    audio_mnist_nn_objective = lambda weight_matrix: nn_cost_function_ridge(audio_mnist_training_data,\\\n",
    "                                                               weight_matrix,\\\n",
    "                                                               audio_mnist_OHV,\\\n",
    "                                                                audio_mnist_ridge_regularisation_parameter)\n",
    "\n",
    "    audio_mnist_nn_gradient = lambda weight_matrix: nn_back_propagation(audio_mnist_training_data,\\\n",
    "                                                                 weight_matrix,\\\n",
    "                                                                 audio_mnist_OHV)\n",
    "\n",
    "    audio_mnist_nn_step = 3.9/(np.linalg.norm(audio_mnist_training_data_matrix, 2) ** 2)\n",
    "\n",
    "    audio_mnist_nn_initial_weight_matrix = np.empty(no_of_layers, dtype = np.ndarray)\n",
    "\n",
    "    for i in range(no_of_layers - 1):\n",
    "        audio_mnist_nn_initial_weight_matrix[i] = np.zeros((audio_mnist_training_data.shape[1]+1,\\\n",
    "                                                     audio_mnist_training_data.shape[1]))\n",
    "\n",
    "    audio_mnist_nn_initial_weight_matrix[no_of_layers - 1] = np.zeros((audio_mnist_training_data.shape[1]+1,\\\n",
    "                                                                       audio_mnist_OHV.shape[1]))\n",
    "\n",
    "    audio_mnist_nn_optimal_weights = gradient_descent(\\\n",
    "                                        audio_mnist_nn_objective, \\\n",
    "                                        audio_mnist_nn_gradient, \\\n",
    "                                        audio_mnist_nn_initial_weight_matrix,\\\n",
    "                                        step_size = audio_mnist_nn_step, \\\n",
    "                                        no_of_iterations=2000, \\\n",
    "                                        print_output=1000)[0]\n",
    "\n",
    "\n",
    "    audio_mnist_nn_accuracy_rate = classification_accuracy(np.concatenate(input_labels),\n",
    "                                                           nn_prediction_function(audio_mnist_training_data, \\\n",
    "                                                                            audio_mnist_nn_optimal_weights))\n",
    "\n",
    "    \n",
    "    labels = nn_prediction_function(audio_mnist_training_data_matrix, audio_mnist_nn_optimal_weights)\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172f9058",
   "metadata": {},
   "source": [
    "## Multinomial Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3786fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multinomial_logistic_regression_cost_function(data_matrix, weight_matrix,\n",
    "                                                  one_hot_vector_encodings):\n",
    "    prediction = data_matrix@weight_matrix\n",
    "    return (1/len(data_matrix)*(np.sum(np.log(np.sum(np.exp(prediction), axis=1)) - np.sum(one_hot_vector_encodings\\\n",
    "                                                                            *prediction, axis=1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2cb4be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multinomial_logistic_regression_gradient(data_matrix, weight_matrix,\n",
    "                                             one_hot_vector_encodings):\n",
    "    output = (1/len(data_matrix))*(data_matrix.T@(softmax_function(data_matrix@weight_matrix, axis=1) - \\\n",
    "                                               one_hot_vector_encodings))\n",
    "#     print(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f437839",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multinomial_prediction_function(data_matrix, weight_matrix):\n",
    "    return np.argmax(data_matrix@weight_matrix, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "904895b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 200/2000, objective = 0.5451678844894882.\n",
      "Iteration 400/2000, objective = 0.3440759627875746.\n",
      "Iteration 600/2000, objective = 0.2638097624142257.\n",
      "Iteration 800/2000, objective = 0.218831518095195.\n",
      "Iteration 1000/2000, objective = 0.18929079559559858.\n",
      "Iteration 1200/2000, objective = 0.16804109955129592.\n",
      "Iteration 1400/2000, objective = 0.15183825700013626.\n",
      "Iteration 1600/2000, objective = 0.13897645581661028.\n",
      "Iteration 1800/2000, objective = 0.1284618170671333.\n",
      "Iteration 2000/2000, objective = 0.11967008369792075.\n",
      "Iteration completed after 2000/2000, objective = 0.11967008369792075.\n"
     ]
    }
   ],
   "source": [
    "# Multinomial Logistic Regression Training Only\n",
    "\n",
    "audio_mnist_training_data = standardise(audio_mnist_training_mfccs)[0]\n",
    "\n",
    "audio_mnist_training_data_matrix = linear_regression_data(audio_mnist_training_data)\n",
    "\n",
    "audio_mnist_OHV = one_hot_vector_encoding_mnist(audio_mnist_training_labels)\n",
    "\n",
    "audio_mnist_objective = lambda weight_matrix: multinomial_logistic_regression_cost_function(\\\n",
    "                                                                                audio_mnist_training_data_matrix,\\\n",
    "                                                                                weight_matrix,\\\n",
    "                                                                                audio_mnist_OHV)\n",
    "\n",
    "\n",
    "audio_mnist_gradient = lambda weight_matrix: multinomial_logistic_regression_gradient(\\\n",
    "                                                                                audio_mnist_training_data_matrix,\\\n",
    "                                                                                weight_matrix,\\\n",
    "                                                                                audio_mnist_OHV)\n",
    "\n",
    "audio_mnist_initial_weights = np.zeros(shape=(len(audio_mnist_training_data_matrix.T),10))\n",
    "\n",
    "audio_mnist_step_size = (3.9*len(audio_mnist_training_data_matrix))/(np.linalg.norm(\\\n",
    "                                                                            audio_mnist_training_data_matrix)**2)\n",
    "\n",
    "audio_mnist_optimal_weights, audio_mnist_objective_values = gradient_descent_v2(audio_mnist_objective, \\\n",
    "                                                                                audio_mnist_gradient, \\\n",
    "                                                                                audio_mnist_initial_weights,\\\n",
    "                                                                                audio_mnist_step_size, \\\n",
    "                                                                                no_of_iterations=2000, \\\n",
    "                                                                                print_output=200, \\\n",
    "                                                                                tolerance=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29f3be79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The classification accuracy for the audio_mnist training dataset is 98.58333333333333 %.\n"
     ]
    }
   ],
   "source": [
    "# Multinomial Regression Classification Accuracy Training Only\n",
    "\n",
    "audio_mnist_recovered_labels = multinomial_prediction_function(audio_mnist_training_data_matrix, \\\n",
    "                                                               audio_mnist_optimal_weights)\n",
    "\n",
    "audio_mnist_classification_accuracy = classification_accuracy(np.concatenate(audio_mnist_training_labels), \\\n",
    "                                                              audio_mnist_recovered_labels)\n",
    "\n",
    "print(\"The classification accuracy for the audio_mnist training dataset is {p} %.\".format(\\\n",
    "                                                                    p=100 * audio_mnist_classification_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "782325c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The classification accuracy for the audio_mnist testing dataset is 94.33333333333334 %.\n"
     ]
    }
   ],
   "source": [
    "# Multinomial Regression Classification Accuracy Testing\n",
    "\n",
    "audio_mnist_testing_data = standardise(audio_mnist_testing_mfccs)[0]\n",
    "\n",
    "audio_mnist_testing_data_matrix = linear_regression_data(audio_mnist_testing_data)\n",
    "\n",
    "audio_mnist_recovered_labels = multinomial_prediction_function(audio_mnist_testing_data_matrix, \\\n",
    "                                                               audio_mnist_optimal_weights)\n",
    "\n",
    "audio_mnist_classification_accuracy_test = classification_accuracy(np.concatenate(audio_mnist_testing_labels), \\\n",
    "                                                              audio_mnist_recovered_labels)\n",
    "\n",
    "print(\"The classification accuracy for the audio_mnist testing dataset is {p} %.\".format(\\\n",
    "                                                                    p=100 * audio_mnist_classification_accuracy_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7621330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 200/2000, objective = 296.77807354488596.\n",
      "Iteration 400/2000, objective = 183.89462896727153.\n",
      "Iteration 600/2000, objective = 137.20333571002507.\n",
      "Iteration 800/2000, objective = 110.60128425983858.\n",
      "Iteration 1000/2000, objective = 93.05960809784006.\n",
      "Iteration 1200/2000, objective = 80.4851920780418.\n",
      "Iteration 1400/2000, objective = 70.96986527004847.\n",
      "Iteration 1600/2000, objective = 63.48859706054527.\n",
      "Iteration 1800/2000, objective = 57.436756424822555.\n",
      "Iteration 2000/2000, objective = 52.433275034760676.\n",
      "Iteration completed after 2000/2000, objective = 52.433275034760676.\n"
     ]
    }
   ],
   "source": [
    "# Multinomial Neural Network\n",
    "\n",
    "no_of_layers = 1\n",
    "\n",
    "audio_mnist_training_data = standardise(audio_mnist_training_mfccs)[0]\n",
    "\n",
    "audio_mnist_training_data_matrix = linear_regression_data(audio_mnist_training_data)\n",
    "\n",
    "audio_mnist_OHV = one_hot_vector_encoding_mnist(audio_mnist_training_labels)\n",
    "\n",
    "audio_mnist_nn_objective = lambda weight_matrix: nn_cost_function(audio_mnist_training_data,\\\n",
    "                                                           weight_matrix,\\\n",
    "                                                           audio_mnist_OHV)\n",
    "\n",
    "audio_mnist_nn_gradient = lambda weight_matrix: nn_back_propagation(audio_mnist_training_data,\\\n",
    "                                                             weight_matrix,\\\n",
    "                                                             audio_mnist_OHV)\n",
    "\n",
    "audio_mnist_nn_step = 3.9/(np.linalg.norm(audio_mnist_training_data_matrix, 2) ** 2)\n",
    "\n",
    "audio_mnist_nn_initial_weight_matrix = np.empty(no_of_layers, dtype = np.ndarray)\n",
    "\n",
    "audio_mnist_nn_initial_weight_matrix[no_of_layers - 1] = np.zeros((audio_mnist_training_data.shape[1]+1,\\\n",
    "                                                                   audio_mnist_OHV.shape[1]))\n",
    "\n",
    "audio_mnist_nn_optimal_weights = gradient_descent(audio_mnist_nn_objective,audio_mnist_nn_gradient,\\\n",
    "                                                  audio_mnist_nn_initial_weight_matrix,\\\n",
    "                                                  step_size=audio_mnist_nn_step, \\\n",
    "                                                  no_of_iterations=2000, print_output=200)[0]\n",
    "\n",
    "audio_mnist_nn_accuracy_rate = classification_accuracy(np.concatenate(audio_mnist_training_labels),\n",
    "                                                       nn_prediction_function(audio_mnist_training_data, \\\n",
    "                                                                        audio_mnist_nn_optimal_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "736f0888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The one layered neural network successfully classified 99.92 % of training data\n"
     ]
    }
   ],
   "source": [
    "print(\"The one layered neural network successfully classified {acc:2.2f} % of training data\"\\\n",
    "      .format(acc = 100 * audio_mnist_nn_accuracy_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af057555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The one layered neural network successfully classified 95.00 % of testing data\n"
     ]
    }
   ],
   "source": [
    "audio_mnist_testing_data = standardise(audio_mnist_testing_mfccs)[0]\n",
    "\n",
    "audio_mnist_testing_data_matrix = linear_regression_data(audio_mnist_testing_data)\n",
    "\n",
    "audio_mnist_nn_accuracy_rate_testing = classification_accuracy(np.concatenate(audio_mnist_testing_labels),\n",
    "                                                       nn_prediction_function(audio_mnist_testing_data, \\\n",
    "                                                                        audio_mnist_nn_optimal_weights))\n",
    "\n",
    "print(\"The one layered neural network successfully classified {acc:2.2f} % of testing data\"\\\n",
    "      .format(acc = 100 * audio_mnist_nn_accuracy_rate_testing))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514e2302",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca1c761a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_multinomial_logistic_regression_cost_function(data_matrix,\n",
    "                                                   weights, OHV,\n",
    "                                                   regularisation_parameter):\n",
    "    \n",
    "    return multinomial_logistic_regression_cost_function(data_matrix, weights,OHV) + \\\n",
    "            (regularisation_parameter/2)*np.linalg.norm(weights)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "660a1163",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_multinomial_logistic_regression_gradient(data_matrix,\n",
    "                                              weights, OHV, regularisation_parameter):\n",
    "    \n",
    "    return multinomial_logistic_regression_gradient(data_matrix, weights,OHV) + regularisation_parameter*weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f63f1272",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2000/2000, objective = 0.11967008411295696.\n",
      "Iteration completed after 2000/2000, objective = 0.11967008411295696.\n",
      "Iteration 2000/2000, objective = 0.11967008784828301.\n",
      "Iteration completed after 2000/2000, objective = 0.11967008784828301.\n",
      "Iteration 2000/2000, objective = 0.11967012520154109.\n",
      "Iteration completed after 2000/2000, objective = 0.11967012520154109.\n",
      "Iteration 2000/2000, objective = 0.11967049873391165.\n",
      "Iteration completed after 2000/2000, objective = 0.11967049873391165.\n",
      "Iteration 2000/2000, objective = 0.11967423403657505.\n",
      "Iteration completed after 2000/2000, objective = 0.11967423403657505.\n",
      "Iteration 2000/2000, objective = 0.11971158495912146.\n",
      "Iteration completed after 2000/2000, objective = 0.11971158495912146.\n",
      "Iteration 2000/2000, objective = 0.12008488391001451.\n",
      "Iteration completed after 2000/2000, objective = 0.12008488391001451.\n",
      "Iteration 2000/2000, objective = 0.12379697943804854.\n",
      "Iteration completed after 2000/2000, objective = 0.12379697943804854.\n",
      "Iteration 2000/2000, objective = 0.1589551238426235.\n",
      "Iteration completed after 2000/2000, objective = 0.1589551238426235.\n",
      "Iteration 2000/2000, objective = 0.39358223718076385.\n",
      "Iteration completed after 2000/2000, objective = 0.39358223718076385.\n",
      "[0.01416667 0.01416667 0.01416667 0.01416667 0.01416667 0.01416667\n",
      " 0.01416667 0.01416667 0.01416667 0.01791667]\n"
     ]
    }
   ],
   "source": [
    "# OPTIMAL ALPHA GRID SEARCH\n",
    "mnist_regularisation_parameter_grid = np.array([1e-10,1e-9,1e-8,1e-7,1e-6,1e-5,1e-4,1e-3,1e-2,1e-1])\n",
    "\n",
    "mnist_classification_error = lambda regularisation_parameter: 1 - classification_accuracy(\\\n",
    "                                np.concatenate(audio_mnist_training_labels),\\\n",
    "                                MNIST_ridge_multinomial_logistic_model_function(\\\n",
    "                                audio_mnist_training_mfccs,audio_mnist_training_labels,regularisation_parameter))\n",
    "\n",
    "mnist_optimal_regularisation_parameter = grid_search(mnist_classification_error, mnist_regularisation_parameter_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7efa8b9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 200/2000, objective = 0.5451678845738082.\n",
      "Iteration 400/2000, objective = 0.3440759629352873.\n",
      "Iteration 600/2000, objective = 0.26380976261093164.\n",
      "Iteration 800/2000, objective = 0.21883151833308084.\n",
      "Iteration 1000/2000, objective = 0.1892907958696583.\n",
      "Iteration 1200/2000, objective = 0.16804109985798218.\n",
      "Iteration 1400/2000, objective = 0.15183825733675965.\n",
      "Iteration 1600/2000, objective = 0.13897645618103374.\n",
      "Iteration 1800/2000, objective = 0.1284618174576015.\n",
      "Iteration 2000/2000, objective = 0.11967008411295696.\n",
      "Iteration completed after 2000/2000, objective = 0.11967008411295696.\n"
     ]
    }
   ],
   "source": [
    "# Ridge Multinomial Logistic Regression \n",
    "audio_mnist_ridge_regularisation_parameter = mnist_optimal_regularisation_parameter\n",
    "\n",
    "audio_mnist_data = standardise(audio_mnist_training_mfccs)[0]\n",
    "\n",
    "audio_mnist_ridge_training_data_matrix = linear_regression_data(audio_mnist_data) \n",
    "\n",
    "audio_mnist_OHV = one_hot_vector_encoding_mnist(audio_mnist_training_labels)\n",
    "\n",
    "audio_mnist_ridge_objective = lambda weight_matrix: ridge_multinomial_logistic_regression_cost_function(\\\n",
    "                                                                  audio_mnist_ridge_training_data_matrix,\\\n",
    "                                                                  weight_matrix,audio_mnist_OHV,\\\n",
    "                                                                  audio_mnist_ridge_regularisation_parameter) \n",
    "\n",
    "audio_mnist_ridge_gradient = lambda weight_matrix: ridge_multinomial_logistic_regression_gradient(\\\n",
    "                                                                    audio_mnist_ridge_training_data_matrix,\\\n",
    "                                                                    weight_matrix,audio_mnist_OHV, \\\n",
    "                                                                    audio_mnist_ridge_regularisation_parameter) \n",
    "\n",
    "audio_mnist_ridge_initial_weights = np.zeros(shape=(len(audio_mnist_ridge_training_data_matrix.T),10)) \n",
    "\n",
    "audio_mnist_ridge_step_size = (3.9*len(audio_mnist_ridge_training_data_matrix))/(np.linalg.norm(\\\n",
    "                                                                        audio_mnist_ridge_training_data_matrix)**2)\n",
    "\n",
    "audio_mnist_ridge_optimal_weights, audio_mnist_ridge_objective_values = gradient_descent(\\\n",
    "                                                                        audio_mnist_ridge_objective, \\\n",
    "                                                                        audio_mnist_ridge_gradient, \\\n",
    "                                                                        audio_mnist_ridge_initial_weights,\\\n",
    "                                                                        step_size = audio_mnist_ridge_step_size, \\\n",
    "                                                                        no_of_iterations=2000, \\\n",
    "                                                                        print_output=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a91c222d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ridge classification accuracy for the audio_mnist_training dataset is 98.58333333333333 %.\n"
     ]
    }
   ],
   "source": [
    "# Ridge Classification Accuracy Testing with Grid Search Optimal Alpha\n",
    "\n",
    "audio_mnist_ridge_recovered_labels = multinomial_prediction_function(audio_mnist_ridge_training_data_matrix, \\\n",
    "                                                                     audio_mnist_ridge_optimal_weights)\n",
    "\n",
    "audio_mnist_ridge_classification_accuracy = classification_accuracy(np.concatenate(audio_mnist_training_labels), \\\n",
    "                                                                    audio_mnist_ridge_recovered_labels)\n",
    "\n",
    "print(\"The ridge classification accuracy for the audio_mnist_training dataset is {p} %.\".format(\n",
    "    p=100 * audio_mnist_ridge_classification_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0fc01e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The classification accuracy for the audio_mnist_testing dataset is 94.33333333333334 %.\n"
     ]
    }
   ],
   "source": [
    "# Ridge Classification Accuracy Testing with Grid Search Optimal Alpha\n",
    "\n",
    "audio_mnist_testing_data = standardise(audio_mnist_testing_mfccs)[0]\n",
    "\n",
    "audio_mnist_ridge_testing_data_matrix = linear_regression_data(audio_mnist_testing_data)\n",
    "\n",
    "audio_mnist_ridge_recovered_labels = multinomial_prediction_function(audio_mnist_ridge_testing_data_matrix, \\\n",
    "                                                               audio_mnist_ridge_optimal_weights)\n",
    "\n",
    "audio_mnist_ridge_classification_accuracy_test = classification_accuracy(np.concatenate(audio_mnist_testing_labels), \\\n",
    "                                                              audio_mnist_ridge_recovered_labels)\n",
    "\n",
    "print(\"The classification accuracy for the audio_mnist_testing dataset is {p} %.\".format(\\\n",
    "                                                             p=100 * audio_mnist_ridge_classification_accuracy_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0a96e652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration completed after 2000/2000, objective = [[52.43327503 52.43327503 52.43327503 ... 52.43327503 52.43327503\n",
      "  52.43327503]\n",
      " [52.43327503 52.43327503 52.43327503 ... 52.43327503 52.43327503\n",
      "  52.43327503]\n",
      " [52.43327503 52.43327503 52.43327503 ... 52.43327503 52.43327503\n",
      "  52.43327503]\n",
      " ...\n",
      " [52.43327503 52.43327503 52.43327503 ... 52.43327503 52.43327503\n",
      "  52.43327503]\n",
      " [52.43327503 52.43327503 52.43327503 ... 52.43327503 52.43327503\n",
      "  52.43327503]\n",
      " [52.43327503 52.43327503 52.43327503 ... 52.43327503 52.43327503\n",
      "  52.43327503]].\n"
     ]
    }
   ],
   "source": [
    "# Ridge Regularised Neural Network\n",
    "\n",
    "no_of_layers = 1\n",
    "\n",
    "audio_mnist_ridge_regularisation_parameter = mnist_optimal_regularisation_parameter\n",
    "\n",
    "audio_mnist_training_data = standardise(audio_mnist_training_mfccs)[0]\n",
    "\n",
    "audio_mnist_training_data_matrix = linear_regression_data(audio_mnist_training_data)\n",
    "\n",
    "audio_mnist_OHV = one_hot_vector_encoding_mnist(audio_mnist_training_labels)\n",
    "\n",
    "\n",
    "audio_mnist_nn_objective = lambda weight_matrix: nn_cost_function_ridge(audio_mnist_training_data,\\\n",
    "                                                           weight_matrix,\\\n",
    "                                                           audio_mnist_OHV,\\\n",
    "                                                            audio_mnist_ridge_regularisation_parameter)\n",
    "\n",
    "audio_mnist_nn_gradient = lambda weight_matrix: nn_back_propagation(audio_mnist_training_data,\\\n",
    "                                                             weight_matrix,\\\n",
    "                                                             audio_mnist_OHV)\n",
    "\n",
    "audio_mnist_nn_step = 3.9/(np.linalg.norm(audio_mnist_training_data_matrix, 2) ** 2)\n",
    "\n",
    "audio_mnist_nn_initial_weight_matrix = np.empty(no_of_layers, dtype = np.ndarray)\n",
    "\n",
    "for i in range(no_of_layers - 1):\n",
    "    audio_mnist_nn_initial_weight_matrix[i] = np.zeros((audio_mnist_training_data.shape[1]+1,\\\n",
    "                                                 audio_mnist_training_data.shape[1]))\n",
    "    \n",
    "audio_mnist_nn_initial_weight_matrix[no_of_layers - 1] = np.zeros((audio_mnist_training_data.shape[1]+1,\\\n",
    "                                                                   audio_mnist_OHV.shape[1]))\n",
    "\n",
    "audio_mnist_ridge_nn_optimal_weights = gradient_descent(\\\n",
    "                                    audio_mnist_nn_objective, \\\n",
    "                                    audio_mnist_nn_gradient, \\\n",
    "                                    audio_mnist_nn_initial_weight_matrix,\\\n",
    "                                    step_size = audio_mnist_nn_step, \\\n",
    "                                    no_of_iterations=2000, \\\n",
    "                                    print_output=20000)[0]\n",
    "\n",
    "\n",
    "audio_mnist_ridge_nn_accuracy_rate = classification_accuracy(np.concatenate(audio_mnist_training_labels),\n",
    "                                                       nn_prediction_function(audio_mnist_training_data, \\\n",
    "                                                                        audio_mnist_nn_optimal_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ecacdf38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The one layered neural network successfully classified 99.92 % of training data\n"
     ]
    }
   ],
   "source": [
    "print(\"The one layered neural network successfully classified {acc:2.2f} % of training data\"\\\n",
    "      .format(acc = 100 * audio_mnist_ridge_nn_accuracy_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7f7a1ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The one layered neural network successfully classified 95.00 % of testing data\n"
     ]
    }
   ],
   "source": [
    "audio_mnist_testing_data = standardise(audio_mnist_testing_mfccs)[0]\n",
    "\n",
    "audio_mnist_testing_data_matrix = linear_regression_data(audio_mnist_testing_data)\n",
    "\n",
    "audio_mnist_ridge_nn_accuracy_rate_testing = classification_accuracy(np.concatenate(audio_mnist_testing_labels),\n",
    "                                                       nn_prediction_function(audio_mnist_testing_data, \\\n",
    "                                                                        audio_mnist_nn_optimal_weights))\n",
    "\n",
    "print(\"The one layered neural network successfully classified {acc:2.2f} % of testing data\"\\\n",
    "      .format(acc = 100 * audio_mnist_nn_accuracy_rate_testing))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9c36e6",
   "metadata": {},
   "source": [
    "## LASSO Multinomial Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b04a79dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_logistic_regression_cost_function(data_matrix, weight_matrix,one_hot_vector_encodings,\n",
    "                                            regularisation_parameter):\n",
    "    return  multinomial_logistic_regression_cost_function(data_matrix, weight_matrix,one_hot_vector_encodings) + \\\n",
    "            regularisation_parameter*np.sum(np.abs(weight_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "16f7498a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multinomial_logistic_regression_gradient(data_matrix, weight_matrix,\n",
    "                                             one_hot_vector_encodings):\n",
    "    output = (1/len(data_matrix))*(data_matrix.T@(softmax_function(data_matrix@weight_matrix, axis=1) - \\\n",
    "                                               one_hot_vector_encodings))\n",
    "#     print(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1bfddbe6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2000/2000, objective = 0.11962971482807293.\n",
      "Iteration completed after 2000/2000, objective = 0.11967010408173288.\n",
      "Iteration 2000/2000, objective = 0.11962989830797069.\n",
      "Iteration completed after 2000/2000, objective = 0.11967028753599215.\n",
      "Iteration 2000/2000, objective = 0.11963173310337336.\n",
      "Iteration completed after 2000/2000, objective = 0.11967212207501295.\n",
      "Iteration 2000/2000, objective = 0.11965008069978766.\n",
      "Iteration completed after 2000/2000, objective = 0.11969046710783879.\n",
      "Iteration 2000/2000, objective = 0.11983352054717637.\n",
      "Iteration completed after 2000/2000, objective = 0.11987388132634322.\n",
      "Iteration 2000/2000, objective = 0.12166431484320389.\n",
      "Iteration completed after 2000/2000, objective = 0.12170442091773452.\n",
      "Iteration 2000/2000, objective = 0.13961236283011327.\n",
      "Iteration completed after 2000/2000, objective = 0.1396501075463546.\n",
      "Iteration 2000/2000, objective = 0.2889844523487539.\n",
      "Iteration completed after 2000/2000, objective = 0.2890106709860738.\n",
      "Iteration 2000/2000, objective = 0.9832053365788334.\n",
      "Iteration completed after 2000/2000, objective = 0.983227941801903.\n",
      "Iteration 2000/2000, objective = 2.2797421885959332.\n",
      "Iteration completed after 2000/2000, objective = 2.279743463771098.\n",
      "[0.01416667 0.01416667 0.01416667 0.01416667 0.01416667 0.01416667\n",
      " 0.01416667 0.01583333 0.05125    0.56375   ]\n"
     ]
    }
   ],
   "source": [
    "# OPTIMAL ALPHA GRID SEARCH\n",
    "mnist_regularisation_parameter_grid = np.array([1e-10,1e-9,1e-8,1e-7,1e-6,1e-5,1e-4,1e-3,1e-2,1e-1])\n",
    "\n",
    "mnist_classification_error = lambda regularisation_parameter: 1 - classification_accuracy(\\\n",
    "                                np.concatenate(audio_mnist_training_labels),\\\n",
    "                                MNIST_LASSO_multinomial_logistic_model_function(\\\n",
    "                                audio_mnist_training_mfccs,audio_mnist_training_labels,regularisation_parameter))\n",
    "\n",
    "mnist_optimal_regularisation_parameter = grid_search(mnist_classification_error, mnist_regularisation_parameter_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0421928b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 200/2000, objective = 0.5433708513147619.\n",
      "Iteration 400/2000, objective = 0.34350977615986716.\n",
      "Iteration 600/2000, objective = 0.2635238556973762.\n",
      "Iteration 800/2000, objective = 0.21865415198544458.\n",
      "Iteration 1000/2000, objective = 0.18916764526181992.\n",
      "Iteration 1200/2000, objective = 0.16794940590673058.\n",
      "Iteration 1400/2000, objective = 0.15176669908377005.\n",
      "Iteration 1600/2000, objective = 0.13891870071236773.\n",
      "Iteration 1800/2000, objective = 0.1284140070808654.\n",
      "Iteration 2000/2000, objective = 0.11962971482807293.\n",
      "Iteration completed after 2000/2000, objective = 0.11967010408173288.\n"
     ]
    }
   ],
   "source": [
    "# LASSO Multinomial Logistic Regression Training Only with Grid Search\n",
    "\n",
    "audio_mnist_LASSO_regularisation_parameter = mnist_optimal_regularisation_parameter\n",
    "\n",
    "audio_mnist_data = standardise(audio_mnist_training_mfccs)[0]\n",
    "\n",
    "audio_mnist_LASSO_training_data_matrix = linear_regression_data(audio_mnist_data) \n",
    "\n",
    "audio_mnist_OHV = one_hot_vector_encoding_mnist(audio_mnist_training_labels)\n",
    "\n",
    "audio_mnist_LASSO_objective = lambda weight_matrix: lasso_logistic_regression_cost_function(\\\n",
    "                                                                  audio_mnist_LASSO_training_data_matrix,\\\n",
    "                                                                  weight_matrix,audio_mnist_OHV,\\\n",
    "                                                                  audio_mnist_LASSO_regularisation_parameter) \n",
    "\n",
    "audio_mnist_LASSO_gradient = lambda weight_matrix: multinomial_logistic_regression_gradient(\\\n",
    "                                                                    audio_mnist_LASSO_training_data_matrix,\\\n",
    "                                                                    weight_matrix,audio_mnist_OHV) \n",
    "\n",
    "audio_mnist_LASSO_initial_weights = np.zeros(shape=(len(audio_mnist_LASSO_training_data_matrix.T),10)) \n",
    "\n",
    "audio_mnist_LASSO_step_size = (3.9*len(audio_mnist_LASSO_training_data_matrix))/(np.linalg.norm(\\\n",
    "                                                                        audio_mnist_LASSO_training_data_matrix)**2)\n",
    "\n",
    "audio_mnist_proximal_map = lambda weight_matrix: soft_thresholding(weight_matrix, \\\n",
    "                                                             audio_mnist_LASSO_regularisation_parameter*\\\n",
    "                                                             audio_mnist_LASSO_step_size)\n",
    "\n",
    "audio_mnist_LASSO_optimal_weights, audio_mnist_LASSO_objective_values = proximal_gradient_descent(\\\n",
    "                                                            audio_mnist_LASSO_objective, \\\n",
    "                                                            audio_mnist_LASSO_gradient, audio_mnist_proximal_map, \\\n",
    "                                                            audio_mnist_LASSO_initial_weights, \\\n",
    "                                                            audio_mnist_LASSO_step_size, \\\n",
    "                                                            no_of_iterations=2000, print_output=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "457616f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The classification accuracy for the audio_mnist_training dataset is 98.58333333333333 %.\n"
     ]
    }
   ],
   "source": [
    "# LASSO Classification Accuracy Training Only\n",
    "\n",
    "audio_mnist_LASSO_recovered_labels = multinomial_prediction_function(audio_mnist_LASSO_training_data_matrix, \\\n",
    "                                                                     audio_mnist_LASSO_optimal_weights)\n",
    "\n",
    "audio_mnist_LASSO_classification_accuracy = classification_accuracy(np.concatenate(audio_mnist_training_labels), \\\n",
    "                                                                    audio_mnist_LASSO_recovered_labels)\n",
    "\n",
    "print(\"The classification accuracy for the audio_mnist_training dataset is {p} %.\".format(\n",
    "    p=100 * audio_mnist_LASSO_classification_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e4a8ffb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The classification accuracy for the audio_mnist_testing dataset is 94.33333333333334 %.\n"
     ]
    }
   ],
   "source": [
    "# LASSO Classification Accuracy Testing\n",
    "\n",
    "audio_mnist_testing_data, _, _ = standardise(audio_mnist_testing_mfccs)\n",
    "\n",
    "audio_mnist_LASSO_testing_data_matrix = linear_regression_data(audio_mnist_testing_data)\n",
    "\n",
    "audio_mnist_LASSO_recovered_labels = multinomial_prediction_function(audio_mnist_LASSO_testing_data_matrix, \\\n",
    "                                                               audio_mnist_LASSO_optimal_weights)\n",
    "\n",
    "audio_mnist_classification_accuracy_test = classification_accuracy(np.concatenate(audio_mnist_testing_labels), \\\n",
    "                                                              audio_mnist_LASSO_recovered_labels)\n",
    "\n",
    "print(\"The classification accuracy for the audio_mnist_testing dataset is {p} %.\".format(\\\n",
    "                                                                    p=100 * audio_mnist_classification_accuracy_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atmospheric-nursing",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "82c3807d111e6f50c87b7ae4cb90f853",
     "grade": false,
     "grade_id": "cell-Q3CodeInfo2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Specify the code for your best data model (linear, polynomial or else) applied to your best weights\n",
    "and assign the result to the output of the function $\\mathtt{MNIST\\_model\\_function}$. The data model has to be created from the argument *inputs*, which is a two-dimensional array where the first dimension equals the number of samples and the second dimension the dimension of the data (900 in case of Audio MNIST). If, for example, your model is a linear basis model applied to a weight matrix *best_weight_matrix*, then your code could look like:\n",
    "\n",
    "```\n",
    "def MNIST_model_function(inputs):\n",
    "    return linear_regression_data(inputs) @ best_weight_matrix\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "armed-pepper",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-24T13:27:08.413652Z",
     "start_time": "2021-12-24T13:27:08.378603Z"
    },
    "deletable": false,
    "hidden": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "37b8be7793e9f0fe5f6f4348846589e0",
     "grade": false,
     "grade_id": "cell-Q3ModelFunctionSolution",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def MNIST_model_function(inputs):\n",
    "    '''Specify your model function. The output has to be a two-dimensional array of size sx1\n",
    "    You are not allowed to include additional input arguments.'''\n",
    "\n",
    "    audio_mnist_data = standardise(inputs)[0]\n",
    "\n",
    "    best_weight_matrix = audio_mnist_ridge_nn_optimal_weights\n",
    "    \n",
    "    labels = nn_prediction_function(audio_mnist_data,best_weight_matrix)\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete-dining",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c8a641a777832e10d4e6e037eb6c72ff",
     "grade": false,
     "grade_id": "cell-Q3ModelFunctionMarking",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The code in the next cell will then evaluate the classification performance of your best classifier\n",
    "when applied to hidden data that is similar (but different) to the MNIST training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "sticky-scroll",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-24T13:27:11.545320Z",
     "start_time": "2021-12-24T13:27:10.638217Z"
    },
    "deletable": false,
    "hidden": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3427e04558ed2a38294c8b6e054b696f",
     "grade": true,
     "grade_id": "cell-Q3ModelFunctionTesting",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The one layered neural network successfully classified 95.00 % of testing data'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def classification_performance_evaluation(inputs, labels):\n",
    "    \n",
    "    recovered_labels = MNIST_model_function(inputs)\n",
    "    \n",
    "    accuracy_rate = classification_accuracy(np.concatenate(labels), recovered_labels)\n",
    "                                                       \n",
    "\n",
    "    return \"The one layered neural network successfully classified {acc:2.2f} % of testing data\"\\\n",
    "      .format(acc = 100 * accuracy_rate)\n",
    "\n",
    "'--------------------------------------------------------------------------------------------------------------------'\n",
    "\n",
    "classification_performance_evaluation(audio_mnist_testing_mfccs, audio_mnist_testing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elementary-petite",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3a5c9ec8e5b546ee44e25689a24f1cd2",
     "grade": false,
     "grade_id": "cell-EndOfProject",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "This completes the MTH786 coding project requirements."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "hide_code_all_hidden": false,
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
